# 前端预备

## url

全名统一资源定位器，形如：http://host[:port]/[source]

* http：超文本传输协议，https中的s是safe的意思
* host：主机域名或IP地址
* port：端口号，默认80
* source：请求的资源

## 标记
* HTML：超文本标记语言

* XML：拓展标记语言

* JSON：JS标记对象，str型的键值对，无注释

* YAML：无类型键值对
  
  * 缩进表示键值关系
  
    如：
  
  ```yaml
  name:
  	formal_name:上海大学
  ```
  
    * -表示并列关系
    
      如：
  
  ```yaml
  name:
      -formal_name:上海大学
      -informal_name:上海市宝山区人民公园
  ```
  
  * | 表示大段文字，# 表示注释
    
      如：
  
  ```yaml 
  text: |  # 上海大学简介
  上海大学（Shanghai University），简称“上大”，是上海市属、国家“211工程”重点建设的综合性大学，教育部与上海市人民政府共建高校，国防科技工业局与上海市人民政府共建高校，世界一流学科建设高校，入选“教育部来华留学示范基地”、“卓越工程师教育培养计划”、“卓越新闻传播人才教育培养计划”、“国家建设高水平大学公派研究生项目”、“111计划”、“上海市首批高水平地方高校建设试点”、“上海市首批深化创新创业教育改革示范高校”。2018年，上海大学在QS世界大学建校50年以下150强排名中列中国大陆第1名；在QS世界大学排名中列中国大陆第16名；在USNews世界大学排名中列中国大陆第34名。
  ```

# 爬虫

网页主要包括三个部分：HTML，CSS，JavaScrip
## HTML
超级文本标记语言（HyperText Markup Language）通过标记符号来标记要显示的网页中的各个部分，浏览器按顺序阅读网页文件，然后根据标记符解释和显示其标记的内容
* 标签
用尖括号包围的关键词，通常成对出现，如<li></li>
* 元素
标签对以及里面的内容，即元素=开始标签+内容+结束标签
* 属性
开始标签中以名称/值对的形式出现的内容

## CSS
CSS是一种定义元素样式结构如字体、颜色、位置等的语言，主要对网页进行格式化的操作
* 选择器
选择对哪些元素进行声明（定义样式结构）
* 声明
对元素进行样式设置，多个选择和声明用;分隔
如{color:red;}

## JavaScrip
多用于Web应用开发，常用来为网页添加动态交互功能，为用户提供更流畅的浏览效果，通常嵌入在HTML中

## 爬虫的流程

* 发起请求（request）
客户端向服务器发出一个请求，包括headers，等待响应
* 获得响应（response）
服务器获得请求后将响应回传给客户端，包括请求的各类内容
* 解析内容
得到的内容可能是二进制文件，json文件，HTML等，需要解析
* 保存数据
将解析的结果保存在文件或数据库里

### Request

发起请求可以用requests库

* 请求方式
  * get
    `requests.get() `获取资源 
  * put
    `requests.put() `请求获得新资源，覆盖原资源 
  * post
    `requests.post() `在原资源之后新增资源 
  * patch
    `requests.patch()` 改变原资源的部分内容 
  * delete
    `requests.delete() `删除原资源 
  
* 请求头（head）
  `requests.head()` 获得头部信息，如User-Agent，host，cookies
  
* 请求体（body）
  请求时的额外数据，如表单

### Response

* 响应状态
`response.status_code `返回状态码，200表示成功响应，404找不到页面，502服务器错误
* 响应头
包含了返回内容的类型，长度等
* 响应体
包含请求的资源

```python
response.text #字符串形式，即url对应的页面内容
response.content #二进制形式
response.encoding # headers中得到的编码方式(charset)，不一定准确
response.apparent_encoding # 内容中获得的编码方式
response.encoding=response.apparent_encoding # 改变编码方式       
response.raise_for_status() #如果状态码不是200，引起HTTPError异常
```

爬虫100次百度首页总时间

```python
import time,requests
start=time.perf_counter()
url='https://baidu.com'
headers={'User-Agent':'Mozilla/5.0'}
for i in range(100):
    try:
        response=requests.get(url=url,headers=headers)
        response.raise_for_status()
        response.encoding=response.apparent_encoding
    
    except:
        print('请求异常')

print(f'爬虫100次百度首页总时间为：{time.perf_counter()-start:.2f}s')
```

爬取京东戴尔G3销售页面

```python
import requests
url='https://item.jd.com/100005322584.html'
headers={'User-Agent':'Mozilla/5.0'}
try:
    response=requests.get(url=url,headers=headers)
    response.raise_for_status()
    response.encoding=response.apparent_encoding
    print(response.text)
except:
    print('请求异常')
```

百度自动搜索

```python
import requests
kv={'wd':'python'}
url='http://www.baidu.com/s?'
try:
    response=requests.get(url=url,params=kv)
    response.raise_for_status()
    response.encoding=response.apparent_encoding
    print(response.request.url)
    print(len(response.text))
except:
    print('请求失败')
```

自动查询IP地址

```python
url='http://m.ip138.com/ip.asp?ip='
def search(ip):
    try:
        response=requests.get(url+ip)
        response.raise_for_status()
        response.encoding=response.apparent_encoding
        return response.text[-500:]
    except:
        return '请求失败'
    
print(search('202.121.199.138'))
```

### 解析方式
响应内容可以是非结构化数据，html，用re正则，BeautifulSoup(BS)，PyQuery，Xpath解析；也可以是结构化数据，json，用json库解析，处理方法是将json格式的数据转化成python数据类型

### re

| 符号  |               含义               |
| :---: | :------------------------------: |
|  \w   |          字母数字下划线          |
|  \d   |               数字               |
|  \s   |              空白符              |
|  \W   |         非字母数字下划线         |
|  \D   |              非数字              |
|  \S   |             非空白符             |
|   ^   |            字符串开头            |
|   $   |            字符串结尾            |
|   .   | 通配符，匹配除了换行符的所有字符 |
|  `*`  |              任意次              |
|   ?   |             0次或1次             |
|   +   |           1次或更多次            |
|  [n]  |        从其中元素选择一个        |
|  {m}  |               m次                |
| {m,k} |             m次到k次             |
| {m,}  |           m次或更多次            |

`re.split(re,string,maxsplit,flag)`
按匹配到的结果分割字符串，可设置分割次数，返回列表
re匹配\d，\w等时，用`r'\d'`避免转义

```python
import re
re.split(r'[1-9]\d{5}','dfg189354 lki457369') 
re.split(r'[1-9]\d{5}','dfg189354 lki457369',maxsplit=1)
```

`re.finditer`

返回可迭代对象

```python
for m in re.finditer(r'[1-9]\d{5}','dfg189354 lki457369'):
    print(m.group())
```

`re.match`

从字符串的起始位置开始匹配，默认大小写敏感

```python
m=re.match(r'[1-9]\d{5}','189354dfg lki457369')
print(m,'\n',m.group())
```

| 表达式 |           含义           |
| :----: | :----------------------: |
|  re.I  |        忽略大小写        |
|  re.S  | 忽略包括换行符在内的空格 |

 ```python
m=re.match(r'\d{6}Dfg','189354dfg lki457369',re.I)
m
 ```

`re.search`

匹配第一个符合的子字符串

```python
re.search(r'Lki\d{6}','189354dfg lki457369',re.I)
```

`m.group()
m.groups()`

分组，获取匹配的结果

```python
m=re.search(r'(\d{6}).*?(\d{6})','189354dfg lki457369').groups()
m

re.search(r'(\d{6}).*?(\d{6})','189354dfg lki457369').group(1)
re.search(r'(\d{6}).*?(\d{6})','189354dfg lki457369').group(2)

m.span() # 匹配的长度
m.end()  
m.start()
```

#### 贪婪和非贪婪

贪婪就是尽可能匹配多的内容，非贪婪可以用`.*?`表示

```python
re.search(r'dfg.*(\d+)lki','dfg189354 457369lki').group(1)
re.search(r'dfg.*?(\d+)lki','dfg189354 457369lki').group(1)
```

`re.findall()`

匹配到符合的所有子字符串，返回列表

```python
import re
html = '''<div id="songs-list">
<h2 class="title">经典老歌</h2>
<p class="introduction">
经典老歌列表
</p>
<ul id="list" class="list-group">
<li data-view="2">一路上有你</li>
<li data-view="7">
<a href="/2.mp3" singer="任贤齐">沧海一声笑</a>
</li>
<li data-view="4" class="active">
<a href="/3.mp3" singer="齐秦">往事随风</a>
</li>
<li data-view="6"><a href="/4.mp3" singer="beyond">光辉岁月</a></li>
<li data-view="5"><a href="/5.mp3" singer="陈慧琳">记事本</a></li>
<li data-view="5">
<a href="/6.mp3" singer="邓丽君">但愿人长久</a>
</li>
</ul>
</div>'''
results = re.findall('<li.*?href="(.*?)".*?singer="(.*?)">(.*?)</a>', html, re.S)
print(results)
print(type(results))
for result in results:
print(result)
print(result[0], result[1], result[2])
```

`re.compile()`

编译，格式化正则

```python
import re
html = '''<div id="songs-list">
<h2 class="title">经典老歌</h2>
<p class="introduction">
经典老歌列表
</p>
<ul id="list" class="list-group">
<li data-view="2">一路上有你</li>
<li data-view="7">
<a href="/2.mp3" singer="任贤齐">沧海一声笑</a>
</li>
<li data-view="4" class="active">
<a href="/3.mp3" singer="齐秦">往事随风</a>
</li>
<li data-view="6"><a href="/4.mp3" singer="beyond">光辉岁月</a></li>
<li data-view="5"><a href="/5.mp3" singer="陈慧琳">记事本</a></li>
<li data-view="5">
<a href="/6.mp3" singer="邓丽君">但愿人长久</a>
</li>
</ul>
</div>'''
pattern = re.compile('<li.*?href="(.*?)".*?singer="(.*?)">(.*?)</a>',re.S)
results = re.findall(pattern, html)
print(results)
print(type(results))
for result in results:
print(result)
print(result[0], result[1],result[2])
```

`re.sub()`

替换

```python
import re
content = "hello 123456 World"
content = re.sub('\d+','',content)
print(content)

content = re.sub('(\d+)',r'\1 7890',content)
print(content)
```

### BeautifulSoup

```python
from bs4 import BeautifulSoup
html = '''
<body>
<header id="header">
<h3 id="name">爬虫</h3>
<title>标题</title>
<div class="bs">
<a href="http://www.baidu.com" target="_blank" rel="nofollow" title="bS"><i
class="fa fa-bs" aria-hidden="true"></i></a>
<a href="http://www.baidu.com" target="_blank" rel="nofollow" title="re">
<i class="fa fa-re" aria-hidden="true"></i></a>
<a href="http://www.baidu.com" target="_blank" rel="nofollow"
title="XPath"><i class="fa fa-XPath" aria-hidden="true"></i></a>
</div>
</header>
</body>
'''
soup = BeautifulSoup(html,'lxml')
# 格式化输出 soup 对象的内容
print(soup.prettify())
# 根据标签名获取整个标签，只能获取第一个
print(soup.li)
# 获取标签的名字
print(soup.title.name)
# 获取标签中的文本
print(soup.title.string)
# 获取标签title的父标标签
print(soup.title.parent.name)
# 获取li标签的子标签
print(soup.li.contents)
# 获取便签的属性值的两种方式
print(soup.li["class"])
print(soup.li.attrs['class'])
# 使用select，css选择器，会得到所有的标签，返回类型是列表
print(soup.select('li'))
# 类名前加.，id名前加#
print(soup.select('.bs'))
# 获取内容
print(soup.select('.bs')[0].get_text())
# 获取属性值
print(soup.select('.bs')[0].attrs['class'])
# 获取li标签下面的子标签
print(soup.select('li > a')[1].get_text())
# 使用find和findall进行查找
print(soup.find('li',attrs={'class':'bs'}))
print(soup.find_all('li',attrs={"class":"bs"})[0]) # 返回类型列表
```

### Xpath

|  表达式  |          描述          |
| :------: | :--------------------: |
| nodename | 选取此节点的所有子节点 |
|    /     |      从根节点选取      |
|    //    |  不考虑位置，直接匹配  |
|    .     |      选取当前节点      |
|    ..    |  选取当前节点的父节点  |
|    @     |        选取属性        |

| 通配符 |        描述        |
| :----: | :----------------: |
|   *    |  匹配任何元素节点  |
|   @*   |  匹配任何属性节点  |
| node() | 匹配任何类型的节点 |

```python
# 导入lxml的etree库
from lxml import etree
data_str = """
<div>
<ul>
<li class="item-0"><a href="link1.html">first item</a></li>
<li class="item-1"><a href="link2.html">second item</a></li>
<li class="item-inactive"><a href="link3.html">third item</a></li>
<li class="item-1"><a href="link4.html">fourth item</a></li>
<li class="item-0"><a href="link5.html">fifth item</a>
</ul>
</div>
"""

# etree.HTML可以将字符串或者bytes转化为Element python对象
html = etree.HTML(data_str)
# print(html)

# 该数据中缺少了一个li标签的闭合标签，tostring可以自动修正HTML代码，补全标签
result = etree.tostring(html)
print(result.decode("utf-8"))
# 获取class =item-1 的 a标签的href属性
result = html.xpath('//li[@class="item-1"]/a/@href')
print(result)
# 获取class =item-1 的 a标签的文本值
result = html.xpath('//li[@class="item-1"]/a/text()')
print(result)
```

### pyquery

首先要初始化PyQuery对象，有三种方法

* 把网址响应初始化

```python
from pyquery import PyQuery as pq

doc = pq(url='http://www.baidu.com')
print(type(doc))
print(doc)
```

* 将HTML文件初始化

```python
doc = pq(filename = './pyquery_demo.html')
print(type(doc))
print(doc)
```

* 把HTML字符串初始化

```python
html = """
<html lang="en">
<head>
<title>PyQuery</title>
</head>
<body>
<ul id="container">
<li class="o1">MM</li>
<li class="o2">MN</li>
<li class="o3">GN</li>
</ul>
</body>
</html>
"""

doc = pq(html)
print(type(doc))
print(doc)
```

pyqurey解析用css选择器来实现，如果要选id前面加`#`，如果选class前面加`.`

```python
html = """
<html lang="en">
<head>
<title>PyQuery</title>
</head>
<body>
<ul id="container">
<li class="o1">MM</li>
<li class="o2 active">MN<a class='o22'>fad</a></li>
<li class="o3">GN</li>
</ul>
</body>
</html>
"""
doc = pq(html)
# 根据标签
print(doc('title'))
print(doc('#container'))
print(doc('.o1'))
# 组合标签
print(doc('.o2 .o22'))   #空格表示子标签
print(doc('.o2.active'))#没有空格表示是在同一个标签里

# 伪类选择器
print(doc('li:nth-child(2)'))
# 根据标签内容获取标签
print(doc("li:contains('MM')"))
#利用find方法
print(doc.find('li'))
# 也可以用.children()查找直接子元素
container = doc.find('#container')
print(container.children())
#.parent()查找对象的父元素
object_2 = doc.find('.o2')
print(object_2.parent())
# .parents（）祖先节点
object_2 = doc.find('.o2')
parent = object_2.parents('#container')
print(parent)
#.siblings()兄弟元素，即同级别的元素，不包括自己
object_2 = doc.find('.o2')
print(object_2.siblings())
# 遍历
lis = doc('li').items() #.items会是一个生成器
print(type(lis))
for li in lis:
    print(li)

# filter() 根据类名、id名得到指定元素
d=pq("<div><p id='1'>test 1</p><p class='2'>test 2</p></div>")
d('p').filter('#1') #返回[<p#1>]
d('p').filter('.2') #返回[<p.2>]
# eq(index) 根据给定的索引号得到指定元素
# 得到第二个p标签内的内容
print(d('p').eq(1).html()) # 返回test2

# 获取属性值
o2 = doc.find('.o2')
print(o2.attr('class'))
#  获取文本值
print(o2.text())
```

### json

#### dumps

将python数据类型转换为json格式的字符串

```python
dict1 = {
'Code': 200,
'Count': 657,
'Posts': [
{
'Id': 0,
'PostId': "1123178321664806912",
'RecruitPostId': 49691
},
{
'Id': 0,
'PostId': "1123178321664806912",
'RecruitPostId': 49691
}
]
}
json_dict = json.dumps(dict1)
print(json_dict)
print(type(json_dict))
```

#### dump

将python数据类型转换为json格式的字符串，并写入文件里

```python
with open('./a.json','w') as f:
	json_dict1 = json.dump(dict1,fp=f)
```

#### loads

将str类型的json文档反序列化为一个python对象

```python
dic = json.loads('{"name":"Tom", "age":23}')
print(dic)
print(type(dic))
```

#### load

将一个json格式的文件序列化为一个python对象

```python
with open("./a.json", "r",encoding='utf-8') as f:
	aa = json.load(f)
	print(aa)
	print(type(aa))
```

## scrapy

### 框架(5模块+2中间件)

* 模块-Engine（核心，不需要修改）
  * 控制模块间的数据流
  * 根据条件触发事件
* 模块-Downloader（不需要修改）
  * 根据请求下载网页
* 模块-Scheduler（不需要修改）
  * 对所有爬取请求进行调度管理
* 中间件-Downloader Middleware
  * 实施Engine,Scheduler和Downloader之间进行用户可配置的控制
  * 修改，丢弃，新增请求和响应信息
* 模块-Spider(用户编写)
  * 解析Downloader返回的响应
  * 产生爬取项（scraped item）
  * 产生额外的爬取请求
* 模块-Item Pipelines(用户编写)
  * 以流水线方式处理Spider产生的爬取项
  * 由一组操作顺序组成，每个操作都是Item Pipeline类型
  * 操作包括：清理，检验，查重爬取项中的HTML数据，将数据存储到数据库中
* 中间件-Spider Middleware(用户编写)
  * 对请求和爬取项进行再处理
  * 修改，新增，丢弃爬取或请求项